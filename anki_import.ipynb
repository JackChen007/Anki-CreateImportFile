{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# \u8bf4\u660e\n",
      "\n",
      "\u8fd9\u4e2anotebook\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u4e00\u4e2ajson\u5bf9\u8c61\u8f6c\u6362\u4e3a\u53ef\u5bfc\u5165Anki\u7684\u6587\u4ef6\u3002\u91cd\u70b9\u5728\u4e8eAnki\u4e2dNoteType\u7684\u8bbe\u8ba1\u3002\u5185\u5bb9\u4e0a\u627f\u63a5explore_all_in_one.ipynb\u3002\n",
      "\n",
      "\u300aGRE\u6838\u5fc3\u8bcd\u6c47\u8003\u6cd5\u7cbe\u6790\u300b\u3001\u300aGRE\u6838\u5fc3\u8bcd\u6c47\u52a9\u8bb0\u4e0e\u7cbe\u7ec3\u300b\u4ee5\u53ca\u4ece\u7f51\u4e0a\u627e\u5230\u7684\u300a\u4e0d\u62e9\u624b\u6bb5\u80cc\u5355\u8bcd\u300b\u5bf9\u5e94NoteType\u4e3aGreWord\u3002\n",
      "\u300aGRE\u9ad8\u5206\u5fc5\u5907\u77ed\u8bed\u642d\u914d\u300b\u5bf9\u5e94NoteTpye\u4e3aGrePhrase\u3002\n",
      "\n",
      "notebook\u6267\u884c\u5b8c\u540e\uff0c\u4f1a\u81ea\u52a8\u751f\u6210\u4e24\u4e2a\u811a\u672c\uff0c\u540d\u5b57\u53c2\u89c1\u53d8\u91cffile_name_greword\uff0cfile_name_grephrase\u3002\u5355\u72ec\u8fd0\u884c\u4e24\u4e2a\u811a\u672c\u4e5f\u53ef\u5b8c\u6210\u8f6c\u6362\uff0c\u53ea\u8981\u6709python\u5c31\u53ef\u4f7f\u7528\u3002\n",
      "\n",
      "\u8f6c\u6362\u51fa\u7684Anki\u5bfc\u5165\u6587\u4ef6\uff0c\u540d\u5b57\u53c2\u89c1\u53d8\u91cfoutput_file_GreWord\uff0coutput_file_GrePhrase\u3002"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run sync_to_file_magic_command.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "file_name_greword = 'CreateAnkiImport_GreWord.py'\n",
      "file_name_grephrase = 'CreateAnkiImport_GrePhrase.py'\n",
      "configCreAnkiImpGreWord = file_name_greword\n",
      "configCreAnkiImpGrePhrase = file_name_grephrase\n",
      "configMyHelpers = 'my_helpers.py'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "\u8865\u5145\u4e24\u4e2a\u8f85\u52a9\u51fd\u6570"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%sync_to_file $configMyHelpers\n",
      "def custom_html_element(_str):\n",
      "    \"\"\"\n",
      "    convert the markdown notations in a string to html tags\n",
      "    currently, only two kinds of markdown notation exist in all the strings\n",
      "    ** and *\n",
      "    \"\"\"\n",
      "    formatted_str = _str\n",
      "    # format double asterisk\n",
      "    match_double_asterisk_re = re.compile(u'\\*\\*(.*?)\\*\\*')\n",
      "    # replace **...** with <strong>...</strong>\n",
      "    #formatted_str = match_double_asterisk_re.sub(r'<strong>\\1</strong>', formatted_str)\n",
      "    # replace **...** with <ins>...</ins>\n",
      "    formatted_str = match_double_asterisk_re.sub(r'<ins>\\1</ins>', formatted_str)\n",
      "    # format single asterisk\n",
      "    # replace *...* with <i>...</i>\n",
      "    match_single_asterisk_re = re.compile(u'\\*(.*?)\\*')\n",
      "    formatted_str = match_single_asterisk_re.sub(r'<i>\\1</i>', formatted_str)\n",
      "    return formatted_str"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "%%sync_to_file $configMyHelpers\n",
      "def is_file_and_json_load(file_name_str):\n",
      "    if os.path.isfile(file_name_str):\n",
      "        with codecs.open(file_name_str, 'r', encoding='utf-8') as f:\n",
      "            json_d = json.load(f)\n",
      "        return json_d"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "%%sync_to_file $configCreAnkiImpGreWord $configCreAnkiImpGrePhrase -m o\n",
      "\n",
      "# coding:utf-8\n",
      "import json\n",
      "import codecs\n",
      "import os.path\n",
      "from my_helpers import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "GreWord"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# example\n",
      "test_str = 'to **put an end to**(something planned or previously agreed to)'\n",
      "print custom_html_element(test_str)\n",
      "del test_str"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "%%sync_to_file $configCreAnkiImpGreWord\n",
      "file_name_new3000 = 'new3000_base_d.txt'\n",
      "file_name_zhuji = 'zhuji_base_d.txt'\n",
      "file_name_bzsdbdc = 'base_data\\\\bzsdbdc_dic.txt'\n",
      "output_file_GreWord = 'AnkiImportData_GreWord.txt'\n",
      "new3000_base_d = None\n",
      "zhuji3000_base_d = None\n",
      "bzsdbdc_data = None\n",
      "new3000_base_d = is_file_and_json_load(file_name_new3000)\n",
      "zhuji3000_base_d = is_file_and_json_load(file_name_zhuji)\n",
      "bzsdbdc_data = is_file_and_json_load(file_name_bzsdbdc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "%%sync_to_file $configCreAnkiImpGreWord\n",
      "no_data_new3000 = new3000_base_d is None\n",
      "no_data_zhuji = zhuji3000_base_d is None\n",
      "no_data_bzsdbdc = bzsdbdc_data is None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%sync_to_file $configCreAnkiImpGreWord\n",
      "def convert_to_GreWord():\n",
      "    if no_data_new3000:\n",
      "        print 'New3000 data file does not exists! Nothing can be done...'\n",
      "        return\n",
      "    if no_data_zhuji:\n",
      "        print 'No data of zhuji!'\n",
      "    if no_data_bzsdbdc:\n",
      "        print 'No data of bzsdbdc!'\n",
      "    output_list = []\n",
      "    None_repr = u''\n",
      "    join_by_line_break = u'<br>'.join\n",
      "    replace_with_br = lambda _str: _str.replace('\\n', '<br>')\n",
      "    tag_pos_prefix = ' in_'\n",
      "    for word in new3000_base_d:\n",
      "        # new 3000 part\n",
      "        \"\"\"\n",
      "        the structure of a word of new3000_base_d.txt\n",
      "\n",
      "        {'phon': u\"[\\u02cc\\xe6d'l\\u026ab]\",\n",
      "         'pos': (1, 6),\n",
      "         'usages': [{'ants': u'\\u53cd\\u3000considered, planned, premeditated, rehearsed \\u9884\\u5148\\u8ba1\\u5212\\u7684',\n",
      "                     'ants_d': {'cn': u'\\u9884\\u5148\\u8ba1\\u5212\\u7684',\n",
      "                                'en': u'considered, planned, premeditated, rehearsed ',\n",
      "                                'en_cn': u'considered, planned, premeditated, rehearsed \\u9884\\u5148\\u8ba1\\u5212\\u7684'},\n",
      "                     'der': '',\n",
      "                     'examples': u'content...',\n",
      "                                    'en': u'not bad for an ad-lib comedy routine',\n",
      "                                    'en_cn': u'content...'},\n",
      "                     'exp': u'*adj.* \\u5373\\u5174\\u7684\\uff1amade or done **without previous thought or preparation**',\n",
      "                     'exp_d': {'cn': u'\\u5373\\u5174\\u7684',\n",
      "                               'en': u'made or done **without previous thought or preparation**',\n",
      "                               'en_cn': u'\\u5373\\u5174\\u7684\\uff1amade or done **without previous thought or preparation**'},\n",
      "                     'ph_symbl': u\"[\\u02cc\\xe6d'l\\u026ab]\",\n",
      "                     'pspeech': u'adj.',\n",
      "                     'syns': u'content...'}\n",
      "        \"\"\"\n",
      "        one_new3000_word_d = new3000_base_d[word]\n",
      "        word_pos_L, word_pos_U = one_new3000_word_d['pos']\n",
      "        word_pos = u'L' + unicode(word_pos_L) + u' U' + unicode(word_pos_U)\n",
      "        num_usages = len(one_new3000_word_d['usages'])\n",
      "        usages_tag = unicode(num_usages) + u'_usage'\n",
      "\n",
      "        for usage_index, usage in enumerate(one_new3000_word_d['usages']):\n",
      "            word_phs = usage['ph_symbl'] \n",
      "            word_tags = usages_tag + tag_pos_prefix + 'zaiyaoniming3000'\n",
      "            if not no_data_zhuji:\n",
      "                if word in zhuji3000_base_d:\n",
      "                    word_tags += tag_pos_prefix + 'zhuji3000'\n",
      "            if not no_data_bzsdbdc:\n",
      "                if word in bzsdbdc_data:\n",
      "                    word_tags += tag_pos_prefix + 'bzsdbdc'\n",
      "            usage_index = unicode(usage_index+1)\n",
      "            word_uid = unicode(word) + usage_index\n",
      "            ph_symbl = usage['ph_symbl']\n",
      "            word_Audio = ''\n",
      "            pspeech = usage['pspeech']\n",
      "            exp_en = usage['exp_d']['en']\n",
      "            exp_cn = usage['exp_d']['cn']\n",
      "            exp_en_cn = usage['exp_d']['en_cn']\n",
      "            # combine other explanation\n",
      "            #usage_index_l = range(num_usages)\n",
      "            #usage_index_l.remove(usage_index)\n",
      "            #exp_other = ['**\u8003\u6cd5%d**:'%(i+1) + one_new3000_word_d['usages'][i]['exp_d']['en_cn'] +'\\n' for i in usage_index_l]\n",
      "            # use word_block_str as all explanation\n",
      "            exp_all = one_new3000_word_d['word_block_str']\n",
      "            examples_en = usage['examples_d']['en']\n",
      "            examples_cn = usage['examples_d']['cn']\n",
      "            examples_en_cn = usage['examples_d']['en_cn']\n",
      "            examples_others = ''\n",
      "            ants_en = usage['ants_d']['en']\n",
      "            ants_cn = usage['ants_d']['cn']\n",
      "            ants_en_cn = usage['ants_d']['en_cn']\n",
      "            syns = usage['syns']\n",
      "            # der from the book zaiyaoniming3000\n",
      "            der_new3000 = usage['der']\n",
      "            \n",
      "            # bzsdbdc part\n",
      "            how_to_mem_bzsdbdc = None_repr\n",
      "            if not no_data_bzsdbdc:\n",
      "                if word in bzsdbdc_data:\n",
      "                    how_to_mem_bzsdbdc = bzsdbdc_data[word]['combined']         \n",
      "\n",
      "            # zhuji3000 part\n",
      "            how_to_mem_zhuji3000, eytma_gr, eytma_gr_exp, eytma_cognates = None_repr, None_repr, None_repr, None_repr\n",
      "            '''\n",
      "            the structure of a word of zhuji3000_base_d\n",
      "            {'content': u'[\\u6839] per- [through] + vad [go] + -e [v.], go through, \\u904d\\u5e03 \\u2192 vt. \\u5f25\\u6f2b\\uff0c\\u5145\\u6ee1\\n',\n",
      "            'ety': 'vad, vag, ced',\n",
      "            'etyma_cognates_l': u'pervade, evasive, extravagant, vague, cessation, incessant',\n",
      "            'etyma_group_explanation': u'group explanation content',\n",
      "            'phon': u\"[p\\u0259r've\\u026ad]\",\n",
      "            'pos': u'6, 7',\n",
      "            'summary': u'summary content',\n",
      "            'word': u'pervade'}\n",
      "            '''\n",
      "            if not no_data_zhuji:\n",
      "                if word in zhuji3000_base_d:\n",
      "                    how_to_mem_zhuji3000 = zhuji3000_base_d[word]['content']\n",
      "                    eytma_gr = zhuji3000_base_d[word]['ety']\n",
      "                    eytma_gr_exp = zhuji3000_base_d[word]['etyma_group_explanation']\n",
      "                    eytma_cognates = zhuji3000_base_d[word]['etyma_cognates_l']\n",
      "            mynotes = None_repr\n",
      "            \"\"\"\n",
      "            Anki GreWord Structure\n",
      "            word_uid  word  usage_index  ph_symbl  word_audio  pspeech  mynotes\n",
      "            exp_en exp_cn exp_en_cn exp_all\n",
      "            examples_en examples_cn examples_encn examples_others\n",
      "            ants_en ants_cn ants_encn\n",
      "            syns der_new3000 \n",
      "            how_to_mem_bzsdbdc how_to_mem_zhuji3000 \n",
      "            etyma_group etyma_group_exp etyma_cognates\n",
      "            position tags\n",
      "            \"\"\"\n",
      "            one_line = [word_uid, word, usage_index, ph_symbl, word_Audio, pspeech, mynotes, \n",
      "                        exp_en, exp_cn, exp_en_cn, exp_all, \n",
      "                        examples_en, examples_cn, examples_en_cn, examples_others,\n",
      "                        ants_en, ants_cn, ants_en_cn] +\\\n",
      "                       [syns, der_new3000, how_to_mem_bzsdbdc, how_to_mem_zhuji3000,\n",
      "                        eytma_gr, eytma_gr_exp, eytma_cognates, word_pos, word_tags]\n",
      "            for index, _str in enumerate(one_line):\n",
      "                _str = replace_with_br(collapse_blank_line(_str).strip(' \\n'))\n",
      "                one_line[index] = custom_html_element(_str)\n",
      "            output_list.append(one_line)\n",
      "    output_list.sort(key=lambda x: x[0])\n",
      "    \n",
      "    with codecs.open(output_file_GreWord, 'w', encoding='utf-8') as f:\n",
      "        for one_line in output_list:\n",
      "            one_string = u'\\t'.join(one_line) + '\\n'\n",
      "            f.write(one_string)  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "convert_to_GreWord()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%sync_to_file $file_name_greword -p\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    convert_to_GreWord()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "collapsed": true
     },
     "source": [
      "GrePhrase"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "%%sync_to_file $configCreAnkiImpGrePhrase\n",
      "file_name_duanyu = 'duanyu_base_d.txt'\n",
      "duanyu_base_d = is_file_and_json_load(file_name_duanyu)\n",
      "output_file_GrePhrase = 'AnkiImportData_GrePhrase.txt'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'The structure of duanyu_base_d'\n",
      "pprint(duanyu_base_d['under one\\'s control1'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%sync_to_file $configCreAnkiImpGrePhrase\n",
      "def convert_to_GrePhrase():\n",
      "    with codecs.open(output_file_GrePhrase, 'w', encoding='utf-8') as f:\n",
      "        for phrase_uid, phrase_dict in duanyu_base_d.iteritems():\n",
      "            one_line = [phrase_uid, phrase_dict['phrase'], phrase_dict['usage_index'],\n",
      "                        phrase_dict['en_exp'], phrase_dict['cn_exp'], \n",
      "                        phrase_dict['example'], phrase_dict['gre_example_cn'],\n",
      "                        phrase_dict['gre_example_en']]\n",
      "            one_line = '\\t'.join(one_line) + '\\n'\n",
      "            f.write(one_line)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "convert_to_GrePhrase()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%sync_to_file $file_name_grephrase -p\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    convert_to_GrePhrase()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "! jupyter nbconvert anki_import.ipynb --to markdown\n",
      "! jupyter nbconvert anki_import.ipynb -- to html"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}